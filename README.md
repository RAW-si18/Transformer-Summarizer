# Transformer Summarizer

## Description
Transformer Summarizer is a project that implements a text summarization model using the transformer architecture. The transformer decoder is implemented from scratch, and this project aims to provide a deeper understanding of how transformers work, particularly in the context of natural language processing (NLP) tasks such as text summarization.

Summarization is an important task in NLP and has numerous applications. For example, bots can scrape articles, summarize them, and use sentiment analysis to identify the sentiment about certain topics, such as stocks. This project will teach you to preprocess data, implement DotProductAttention and Causal Attention, understand how attention mechanisms work, build the transformer model, evaluate the model, and use it to summarize text.

## Prerequisites
- Python 3.11

## Installation
1. Clone the repository:
    ```bash
    git clone https://github.com/RAW-si18/Transformer-Summarizer.git
    ```

## Key Features
- **Data Preprocessing:** Functions to preprocess your data for training.
- **DotProductAttention:** Implementation of the Dot Product Attention mechanism.
- **Causal Attention:** Implementation of the Causal Attention mechanism.
- **Transformer Model:** Build the transformer model from scratch.
- **Model Evaluation:** Evaluate the performance of your summarization model.
- **Text Summarization:** Summarize articles or any given text input.

## Acknowledgments
- Thanks to the open-source community for providing valuable resources and tools.
- Inspiration and learning resources were taken from various NLP and deep learning courses and tutorials.